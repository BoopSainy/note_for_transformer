in this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the
transformer model. since the paper attention is all you need by V et al. had been published in 2017, 
the transformer architecture has continued to beat benchmarks in many domains, most importantly in
natural lanugage processing. transformers with an incredible amount of parameters can 

as the hype of the transformer architecture seems not to come to an end in the next years, it is 
important to understand how it works, and have implemented it yourself, which we will do in this 
notebook.

despite the huge success of transformer in nlp, we will not include the nlp domain in our notebook here.
why? firstly, the master ai at uva offers many great nlp courses that will take a closer look at the 
application of transformer architecture in nlp. secondly, assignment 2 takes already a closer look at
language generation on character level, onwhich you could easily apply our transformer architecture. 
finally, and most importantly, there is so much more to the transformer architecture. nlp is the domain
the transformer architecture has been originally proposed for and had the greatest impact on, but is 
also accelerated research in other domains, recently even computer vision. thus, we focus here on what 
makes the transformer and self-attention so powerful in general. in tutorial 15, we will discuss the 
application of transformer in computer vision.

below, we import our standard libraries

the transformer architecture
inthe first part of this notebook, we will implement the transformer architecture by hand
as the architecture is so popular, there are exists a pytorch module nn.transformer and a
tutorial on how to use it for next token prediction. however, we will implement it here ourselves,
to get through to the smallest details.
there are of course many more tutorial out there about attention and transformers. below, we list
a few that are worth exploring if you are interested in the topic and might want yet another 
perspective on the topic after this one

what is attention?
the attention mechanism describes a recent new group of layers in neural networks that has attracted a
lot of interest in the past few years, especially in sequence tasks. there are alot of different 
possible definitions of attention in the literature, but the one we will use here is the following:
the attention mechanism describes a weighted average of (sequence) elements with the wights dynamically
computed based on an input query and elements' keys. 

{我认为（全是瞎猜的），query可以理解为，假设我们给了一组长度为10的sequence，那么，我们理应有10份关注。
比如当t=1的时候，输入为x1，在这样时间下接收到这样的一种输入，我们应该会产生一种特殊的注意力，也就是q1，就好比当我们
读一段话读到第一个自为“给”的时候，这个时候会对某些词汇，比如物品或者物品的接受者产生更多遐想，而对动词减少一些遐想。
接着呢，我们再用这份注意力q1，与sequence中的每个词分别去做相似度计算，与谁更相似就能得到更大的权重值a_i。再将
a_i通过一个softmax层来转换成和为1的概率分布。就得到了最终的权重值a们。最后再用这些权重值去和sequence中的每个词做
加权平均，就能得到第一个词通过注意力机制转换后的第一个输出。
这第一个输出，其实是指当注意力机制在第一个位置看到第一个输入这种情况后，
产生了一份特殊的query，再用第一个query来审视全部输入词后得到的一个加权输出。}

so what does this exactly mean? the goal is to take an average over the features of multiple elements.
howeverm instead of weighting each element equally, we want to weight them depending on their actual
values. in other words, we want to dynamically decide on which inputs we want to "attend" more
than others. in particular, an attention mechanism has usually four parts we neeed to specify:


query: the query is a feature-capture vector that describes what we are looking for in the sequence, i.e. 
what would we maybe want to pay attention to. (UNDER current input and current time*(might position in image))

keys: for each input element, we have a key which is again a feature vector. this feature vector roughly
describes what the element is "offering", or when it might be important. the keys should be designed 
such that we can identify the elements we want to pay attention to based on the query.

values: for each input element, we also have a value vector. this feature vector is the one we want to 
average over.

score function: to rate which elements we want to pay attention to, we need to specify a score function
fattn. the score function takes the query and a key as input, and output the score/attention weight of 
the query-key pair. it is usually implemented by simple similarity metrics like a dot product, or a small
MLP

the weights of the average are calculated by a softmax over all score function outputs. hence, we assign those
value vectors a higher weight whose corresponding key is most similar to the query.
{所以呢，依鄙人之见，对于同一个input xi, 它的key k1, 和 value v1 是很相似的，都是它本身的一种抽象表达，！！！这里很重要的
一点是，它本身的抽象表达，可不仅仅是它是谁-x1是谁，而是x1加上它的时间编码/位置编码后的表达。
所以也就是说，当某一个query已经确定后，它如果强调了“我就想看到'出现在第一时刻’的‘x'，那么'出现在第一时刻的x‘就会被给予更多关注。
而’出现在第7时刻的x1‘可能就不是香饽饽了}


most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined,
and what score function is used. the attention applied inside the transformer architecture is called self-
attention. in self-attention, each sequence element provides a key, a value and query. for each element, we
perform an attention layer where based on its query, 


Scaled Dot Product Attention:
the core concept behind self-attention is the scaled dot product attention. our goal is to have an attention
mechanism with which any element in a sequence can attend to any other while still being efficient to 
compute. 

one aspect we haven't discussed yet is the scaling factor of 1/dk^2, this scaling factor is crucial to 
maintain an appropriate variance of attention values after initialization.


multi-head attention:
the scaled dot product attention allows a network to attend over a sequence. however, often there are multiple 
different aspects a seqiemce element wants to attend to, and a single weighted average is not a good option for
it.
{这里可以理解为，如果一个element input只能拥有一个长query，那么这个长query其实是包含了很多个小层面上的关注的。我们假设x1的抽象表达
为k1，x2的抽象表达为k2，这个时候，q1k1和q1k2的值很可能很接近，但是实际上，我们可以想象一下，其实k1和k2是相差甚远的，只不过可能k1与q1的
一小段相似度很高，而k2则是与q1的另一小段相似度很高，导致q1k1和q1k2的结果相似，但是这两个相似的小段是不同的两小段。

比如q1 = [10, 0, 0, 0, 0, 10]
k1 = [10, 1, 1, 1, 1, 1]
k2 = [1, 1, 1, 1, 1, 10]

此时a1 = a2 = 110， 看似通过q1之后我们需要给x1和x2的相同的注意力权重，也就是说 我们最终的att1 = 0.5*v1 + 0.5*v2。
但是这样存在的问题就是， 当我们细致到v1v2上的每个entry的时候，我们也给了每个entry同样的注意力。
但是实际上，k1的第一个entry远比k2的第一个entry要与q1的第一个entry更相似，它理应获得更多的注意力。

所以，我们引入多头注意力机制，我们要把q1拆分成很多段，q11 q12 q13... 同时也把k1 k2和v1 v2拆分成很多段， 我们将细致地
考量，不同输入x的每一小段值得多少注意力投入，也就是要得到weighted average vt1, weighted average vt2, ... weighted average vtn，
再将这些小段concatenate起来。

}

{
文章的作者其实将multi-head这里综合叙述成了是通过head_num个线性映射得到head_num组Q,K,V。 我们前述的算是一种简单的特殊情况
}
this is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-values triplets 
on the same features. specifically, given a query, key, and value matrix, we transform those into h sub-queries,
sub-keys, and sub-values, which we pass through the scaled dot product attention independently. afterward, we
concatenate the heads and combine them with a final weigh matrix. 

how are we applying a multi
looking at the computation graphy above, a simple but effective implementation is to set the current feature
map in NN, X(- R^BxTxdmodel, as Q, K, and V (B being the batch size, T the sequence length, dmodel the hidden
dimensionality of X). the consecutive weight matrices WQ, WK, and Wv can transform X to the corresponding feature
vectors that represents

our crucial characteristic of the multi-head attention is that it is permutation-equivariant with
respect to its inputs.
{when we dont add position encoding, if we switch two input elements in the sequence, e.g. X1 <-> X2, the
output is exactly the same besides the elements 1 and 2 switched. hence, the multi-head attention is
actually looking at the input not as a sequence, but as a set of elements. this property makes the 
multi-head attention block and the transformer architecture so powerful and widely applicable! but 
what if the order of the input is actually important for solving the task, like language modeling?
the answer is to encode the position in the input features, which we will take a closer look at 
later (topic positional encodings below)}

before moving on to create the transformer architecture, we can compare the self-attention operation with
our other common layer competitors for sequence data: convolutions and recurrent neural networks. below
you can find a table by V et al. on the complexity per layer, the number of sequential operations, and
maximum path length. the complexity is measured by the upper bound of the number of operations to perform,
while the maximum path length represents the maximum number of steps a forward orbackward signal has to 
traverse to reach any other position

next we will look at how to apply the multi-head attention block inside the transformer architecture.
originally, the transformer model was designed for macine translation. hence, it got an encoder-decoder
structure where the encoder takes as input the sentence in the original language and generates an 
attention-based representation. on the other hand, the decoder attends over the encoded information
and generates the translated sentence in an autoregressive manner, as in a standard rnn. while this 
structure is extremely useful for sequence-to-sequence tasks with the necessity of autoregressive decoding,
we will focus here on the encoder part.
{这个作者在这里只关注编码器}
many advances in nlp have been made using pure encoder-based transformer models (if interested, models
include the BERT-family, the vision transformer, and more), and in our tutorial, we will also mainly
focus on the encoder part. if you have understood the encoder architecture, the decoder is a very small
step to implement as well. the full transfomer architecture looks as follow

the encoder consists of N identical blocks that are applied in sequence. taking as input x, it is first
passed through a multi-head attention block as we have implemented above. the output is added to the
original input using a residual connection, and we apply a consecutive Layer Normalization on the sum.
overall, it calculates LayerNorm(x + Multihead(x, x, x)) (x being Q, K, V input to the attention layer).
the residual connection is crucial in the transformer architecture for two reasons:

1. similar to resnets, transformers are designed to be very deep. some models contain more than 24 blocks
in the encoder. hence, the residual connections are crucial for enabling a smooth gradient flow through
the model.

2. without the residual connection, the information about the original sequence is lost. remember that 
the multi-head attention layer ignores the position of elements in a sequence, and can only learn it 
based on the input features. removing the residual connections would mean that this information is lost
after the first attention layer (after initialization), and with a randomly initialized query and key vector

the layer normalization also plays an importatn role in the transformer architecture as it enables faster
training and provides samll regularization. additionallyy, it ensures that the features are in a similar
magnitude among the elements in the sequence. we are not using batch normalization because it depends on 
the batch size which is often small with transformers (they require alot of GPU memory), and BatchNorm
has shown to perform particualy bad in language as the features of words tend to have a much higher variance
(there are many, very rare words which need to be considered for a good distribution estimate).

additionally to the multi-head attention, a small fully connected feed-forward network is added to the
model, which is applied to each position separatly and identically. 
{在多头注意力层结束后，假设我们batch size为1，那么这个时候，我们应该是由seq_len个输出（维度为embed_dim），这个时候，我们
再将每一个输出分别输送进完全一样的fully connected feed-forward network}
specifically, the model uses a linear->relu->linear mlp. the full transformation including the residual 
connection can be expressed as

this mlp adds extra complexity to the model and allows transformations on each sequence element separately.
you can imagine as this allows the model to "post-process" the new information added by the previous
multi-head attention, and prepare it for the next attention block. usually,

finally, after looking at all parts of the encoder architecture, we can start implementing it below.
we first start by implementing a single encoder block. additionally to the layers described above, we 
will add dropout layers in the mlp and on the output of the mlp and multi-head attention for regularization.


based on this block, we can implement a module for the full transformer encoder. additionally to a forward
function that iterates through the sequence of encoder blocks, we also provide a function called 
get_attention_maps. the idea of this function is to return the attention probabilities for all multi-head
attention blocks in the encoder. this helps us in understanding, and in a sense, explaining the model. 
however, the attention probabilities should be interpreted with a grain salt as it does not
necessarily reflect the true interpretation of the model (there is a series of papers about this, including
attention is not explanation and attention is not not explanation).

{作者认为，我们应当对multi-head attention layer得到的attention probability tensor是否能解读model干了什么持保留意见，
现在有很多不同的声音}


positional encoding
we have discussed before that the multi-head attention block is permutation-equivariant, and cannot distinguish
whether an input comes before another one in the sequence or not. in tasks like language understanding,
however, the position is important for interpreting the input words. the position information can therefore
be added via the input features. we could learn a embedding for every possible position, but this would not
generalize to a dynamical input sequence length.
{这种position embedding不能泛化到动态输入长度的sequence上} hence, the better option is to use feature patterns that
the network can identify from the features and potentially generalize to larger sequences.

pe(pos, i) represents the position encoding at position pos in the sequence, and hidden dimensionality i.
these values concatenated for all hidden dimensions, are added to the original input features (in the 
Transformer visualization above, see "positional encoding"), and constitute the position information. we 
distinguish between even (i mod 2 = 0) and uneven(i mod 2 = 1) hidden dimensionalities where we apply
a sine/cosine respectively. the intuition behind this encoding is that you can represent PE(pos+k,:) as 
a linear function of PE(pos, :), which might allow the model to easily attend to relative positions. the
wavelengths in different dimensions range from 2pi to 10000*2pi.

{在transformer encoder里，当输入进入到第一个encoder block前，这组input elements会分别sum上自己的位置编码。
位置编码让模型知道这个序列中elements的位置关系，假设我们batch size为1，然后这组sequence里有20个input elements，
每个elements的维度为30.

也就是说，我们现在有30个position上有element准备输入，然后呢。
位置编码是pos_num和input_dim的函数。
我们会为每个位置上准备一个维度为input_dim的位置编码，再将这个pe_pos与对应位置上的input element相加，送到第一个
encoder block里}

{需要注意的是，因为PE只是input element的位置，以及input element的维度的函数， 所以对于不同的sequence，它们每一个位置上的
PE理应是一样的}

{作者写的positional encoding思路是，
给定element input dim,
然后直接生成一组供所有sequence使用的position encoding matrix，
而且它直接就准备好超常的一个序列，max_pos_length = 一个很大的数
最后我们使用的时候，只需要拿需要的前多少pos就可以了
}

to understand the positional encoding, we can visualize it below. we will generate am image of the position
encoding over hidden dimensionality and position in a sequence


Learning rate warm-up
one commonly used technique for training a transformer is learning rate warm-uup. this means that we 
gradually increase the learning rate from 0 on to our originally specified learning rate in the 
first few iterations. thus, we slowly start learning instead of taking very large steps from the beginning.
in fact training a deep transformer without learning rate warm-up can make the model diverge and achieve
a much worse performance on training and testing. take for instance the following plot by Liu et al.
comparing adam-vanilla (i.e. adam without warm-up) vs adam with a warm-up

clearly, the warm-up is a crucial hyperparameter in transformer architecture. why is it so important?
there are currently two common explanations. first, adam uses the bias correction factors which however 
can lead to a higher variance in the adaptive learning rate

secondly, the iteratively applied layer normalization across layers can lead to very high gradients during the 
first iterations, which can be solved by using pre-layer normalization

nevertheless, many applications and papers still use the original transformer architecture with adam,
because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations.
there are many different schedulers we could use. for instance, the original transformer paper used an
exponential decay scheduler with a warm-up. however, the currently most popular scheduler is the
cosine warm-up scheduler, which combines warm-up with cosine-shaped learning rate decay. we
can implement it below and visualize the learning rate factor over epochs.


{learning rate warm-up, 先让学习率从小到大增长到设定的学习率大小，再让学习率逐渐decay的过程}
