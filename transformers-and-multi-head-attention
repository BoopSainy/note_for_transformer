in this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the
transformer model. since the paper attention is all you need by V et al. had been published in 2017, 
the transformer architecture has continued to beat benchmarks in many domains, most importantly in
natural lanugage processing. transformers with an incredible amount of parameters can 

as the hype of the transformer architecture seems not to come to an end in the next years, it is 
important to understand how it works, and have implemented it yourself, which we will do in this 
notebook.

despite the huge success of transformer in nlp, we will not include the nlp domain in our notebook here.
why? firstly, the master ai at uva offers many great nlp courses that will take a closer look at the 
application of transformer architecture in nlp. secondly, assignment 2 takes already a closer look at
language generation on character level, onwhich you could easily apply our transformer architecture. 
finally, and most importantly, there is so much more to the transformer architecture. nlp is the domain
the transformer architecture has been originally proposed for and had the greatest impact on, but is 
also accelerated research in other domains, recently even computer vision. thus, we focus here on what 
makes the transformer and self-attention so powerful in general. in tutorial 15, we will discuss the 
application of transformer in computer vision.

below, we import our standard libraries

the transformer architecture
inthe first part of this notebook, we will implement the transformer architecture by hand
as the architecture is so popular, there are exists a pytorch module nn.transformer and a
tutorial on how to use it for next token prediction. however, we will implement it here ourselves,
to get through to the smallest details.
there are of course many more tutorial out there about attention and transformers. below, we list
a few that are worth exploring if you are interested in the topic and might want yet another 
perspective on the topic after this one

what is attention?
the attention mechanism describes a recent new group of layers in neural networks that has attracted a
lot of interest in the past few years, especially in sequence tasks. there are alot of different 
possible definitions of attention in the literature, but the one we will use here is the following:
the attention mechanism describes a weighted average of (sequence) elements with the wights dynamically
computed based on an input query and elements' keys. 

{我认为（全是瞎猜的），query可以理解为，假设我们给了一组长度为10的sequence，那么，我们理应有10份关注。
比如当t=1的时候，输入为x1，在这样时间下接收到这样的一种输入，我们应该会产生一种特殊的注意力，也就是q1，就好比当我们
读一段话读到第一个自为“给”的时候，这个时候会对某些词汇，比如物品或者物品的接受者产生更多遐想，而对动词减少一些遐想。
接着呢，我们再用这份注意力q1，与sequence中的每个词分别去做相似度计算，与谁更相似就能得到更大的权重值a_i。再将
a_i通过一个softmax层来转换成和为1的概率分布。就得到了最终的权重值a们。最后再用这些权重值去和sequence中的每个词做
加权平均，就能得到第一个词通过注意力机制转换后的第一个输出。
这第一个输出，其实是指当注意力机制在第一个位置看到第一个输入这种情况后，
产生了一份特殊的query，再用第一个query来审视全部输入词后得到的一个加权输出。}

so what does this exactly mean? the goal is to take an average over the features of multiple elements.
howeverm instead of weighting each element equally, we want to weight them depending on their actual
values. in other words, we want to dynamically decide on which inputs we want to "attend" more
than others. in particular, an attention mechanism has usually four parts we neeed to specify:


query: the query is a feature-capture vector that describes what we are looking for in the sequence, i.e. 
what would we maybe want to pay attention to. (UNDER current input and current time*(might position in image))

keys: for each input element, we have a key which is again a feature vector. this feature vector roughly
describes what the element is "offering", or when it might be important. the keys should be designed 
such that we can identify the elements we want to pay attention to based on the query.

values: for each input element, we also have a value vector. this feature vector is the one we want to 
average over.

score function: to rate which elements we want to pay attention to, we need to specify a score function
fattn. the score function takes the query and a key as input, and output the score/attention weight of 
the query-key pair. it is usually implemented by simple similarity metrics like a dot product, or a small
MLP

the weights of the average are calculated by a softmax over all score function outputs. hence, we assign those
value vectors a higher weight whose corresponding key is most similar to the query.
{所以呢，依鄙人之见，对于同一个input xi, 它的key k1, 和 value v1 是很相似的，都是它本身的一种抽象表达，！！！这里很重要的
一点是，它本身的抽象表达，可不仅仅是它是谁-x1是谁，而是x1加上它的时间编码/位置编码后的表达。
所以也就是说，当某一个query已经确定后，它如果强调了“我就想看到'出现在第一时刻’的‘x'，那么'出现在第一时刻的x‘就会被给予更多关注。
而’出现在第7时刻的x1‘可能就不是香饽饽了}


most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined,
and what score function is used. the attention applied inside the transformer architecture is called self-
attention. in self-attention, each sequence element provides a key, a value and query. for each element, we
perform an attention layer where based on its query, 


Scaled Dot Product Attention:
the core concept behind self-attention is the scaled dot product attention. our goal is to have an attention
mechanism with which any element in a sequence can attend to any other while still being efficient to 
compute. 

one aspect we haven't discussed yet is the scaling factor of 1/dk^2, this scaling factor is crucial to 
maintain an appropriate variance of attention values after initialization.
