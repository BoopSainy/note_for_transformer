
in this tutorial, we will take a closer look at a recent new trend: transformers for computer vision
since Alexey D et al. successfully applied a transformer on a variety of image recognition benchmarks,
there have been an incredible amount of follow-up works showing that cnns might not be optimal 
architecture for computer vision anymore. but how do vision transformer work exactly, and
what benefits and drawbacks do they offer in contrast to cnns? we will answer these questions by
implementing a vision transformer ourselves and train it on the popular, small dataset
cifar10. we will compare these results to the convolutional architectures of t5

# transformers for image classification
# transformers have been originally proposed to process sets since it is a
# permutation-equivariant architecture, i.e., producing the same output permuted
# if the input is permuted. to apply transformers to sequences, we have simply added
# a positional encoding to the input feature vectors, and the model learned by 
# itself what to do with is. so, why not do the same thing on images? this is 
# exactly what alexy dosovitskiy et al. proposed in their paper "An Image is Worth
# 16x16 Words: Transformer for Image Recognition at Scale". Specifically, the Vision
# Transformer is a model for image classification that views images as sequences
# of smaller patches. As a preprocessing step, we split an image of, for example,
# 48x48 pixels into 9 16x16 patches. Each of those patches is considered to be 
# a "word/token" and projected to a feature space. with adding positional encodings
# and a token for classification on top, we can apply a Transformer as usual to
# this sequence and start training it for our task. A nice GIF visualization of the
# architecture is shown below.

# we will walks step by step through the Vision Transformer, and implement all parts 
# by ourselves. first, let's implement the image preprocessing: an image of size NxN
# has to be split into (N/M)^2 patches of size MxM. These represent the input
# words to the Transformer.

{ViT，一张大图需要被等分成多个小图，每个小图被视为language tasks中的一个token，这张大图可以时为是一个token sequence}



{一个我认为还不错的CLS token的解释}：
首先要说明的是，CLS token存在的意义是去做下游的分类任务。那其实呢，就算我们没有CLS token，理论上我们也可以用剩下的tokens们来进行下游的分类。
但是之所以用CLS token，是觉得它更公平的考虑到了sequence里的每个token。为什么这么说，因为假设我们现在有一个长度为64的sequence，并且很极限的，我们的ViT只有一层
transformer encoder，当我们的一个feature token被送入multi-head attention layer后，会分别把这个token送入三个linear projection(or a large linear projection)去得到
它的query key 和 value，那么由于这个query_1是由feature token_1的linear projection得到，那么它多多少少会相似于自己的key key_1，所以最终的output_1也会相应的包含更多的
value_1的信息。
同理于这个sequence中除CLS token以外的所有feature tokens。那么如果我们最后想得到一个vector来做下游的分类任务，当然不要随意的从feature tokens' outputs里选择了，（假设
我们只有一层transformer encoder，这样更直观），因为这个时候每个output都是更偏向于包含自己token的信息。所以我们想找一个vector，它能公平的包含所有feature tokens的信息，
于是我们创造了这个CLS token，它公平包含了每个feature tokens的信息。
当然，也有其他学者采用另一种方式，那就是用最后一层得到的所有feature tokens的output来生成一个最终的vector，这个vector公平包含了所有feature tokens的信息。


read the paper of ViT to get some details about Vision Transformer and some other insights.

"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"

ABSTRACT
while the transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer
vision remain limited. in vision, attention is either applied in conjunction with convolutional networks, or used to replace certain
components of convolutional networks while keeping their overall structure in place. we show that this reliance on cnns is not necessary
and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. when pre-trained
on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), 
Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially
fewer computational resources to rain.

INTRODUCTION
self-attention-based architectures, in particular transformer, have become the model of choice in natural lanuguage processing (NLP). the 
dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset. thanks to transformers'
computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters. with 
the models and datasets growing, there is still no sign of saturating performance.

in computer vision, however, convolutional architectures remain dominant (lecun et al., 1989; krizhevsky et al., 2012; he et al., 2016). 
inspired by nlp successes, multiple works try combining cnn-like architectures with self-attention (wang et al., 2018; carion et al., 2020),
some replacing the convolutions entirely. the latter models, while theoretically efficient, have not yet been scaled effectively on modern
hardware accelerators due to the use of specialized attention patterns. therefore, in large-scale image recognition, classic resnet like 
architectures are still state of the art.

inspired by the transformer scaling successes in nlp, we experiment with applying a standard transformer directly to images, with the fewest
possible modifications. to do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input
to a transformer. image patches are treated the same way as tokens (words) in an nlp application. we train the model on image claissfication
in supervised fashion.

when trained on mid-sized datasets such as imagenet without strong regularization, these models yield modest accuracies of a few percentage 
points below resnet of comparable size. this seemingly discouraging outcome may be expected: transformers lack some of the inductive biases
inherent to cnns, such as translation equivariance and locality, and therefore do not generalized well when trained on insufficient
amounts of data.

however, the picture changes if the models are trained on larger datasets (14m-300m images). we find that large scale training trumps inductive
bias. our vision transformer attains excellent results when pre-trained at sufficient scale and transferred to taks with fewer datapoints.
when pre-trained on the public imagenet-21k dataset or the in-house jft-300m dataset, vit apporaches or beats state of the art on multiple
image recognition benchmarks. in particular, the best model reaches the accuracy of 88.55% on imagenet, 90.72% on imagenet-real, 94.55% on
CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.

RELATED WORK
transformers were proposed by v et al. for machine translation, and have since become the state of the art method in many nlp tasks.
large transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT uses a denosising self-
supervised pre-trained task, while the GPT line of work uses language modeling as its pre-training task.
...

METHOD
in model design we follow the original transformer as closely as possible. an advantage of this intentionally simple setup is that scalable nlp
transformer architectures - and tehir efficient implementations -can be used almost out of the box.

VISION TRANSFORMER(ViT)
an overview of the model is depicted in figure 1. the standard transformer receives as input a 1d sequence of token embeddings. to handle 2d images,
we reshape the image x (- R^HxWxC into a sequence of flattened 2d patches x_p (- R^Nx(P^2*C), where (H, W) is the resolution of the original
image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P^2 is the resulting number of patches, which also 
serves as the effective input sequence length for the transformer. the transformer uses constant latent vector size D through all of its layers, 
so we flatten the patches and map to D dimensions with a trainable linear projection (eq.1). we refer to the output this projection as the patch
embeddings.

similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches (z^0_0 = x_class),  whose state at the output
of the transformer encoder (z^0_L) serves as the image representation y. both during pre-training and fine-tuning, a classification head
is attached to z^0_L. the classification head is implemented by a mlp with one hidden layer at pre-training time and by a single linear
layer at fine-tuning time.

position embeddings are added to the patch embeddings to retain positional information. we use standard learnable 1d position embeddings, 
since we have not observed significant performance gains from using more advanced 2d-aware position embeddings (appendix d4). the resulting
sequence of embedding vectors serves as input to the encoder.

the transformer encoder consists of alternating layers of multiheaded self-attention and mlp blocks. layernorm (LN) is applied before every block,
and residual connections after every block.

the mlp contains two layers with a GELU non-linerity

inductive bias. we note that vision transformer has much less image-specific inductive bias than cnns. in cnns, locality, two-dimensional 
neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. in vit, only mlp layers 
are local and translationally equivariant, while the self-attention layers are global. the two-dimensional neighborhood structure is used very
sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position

hybrid architecture:
as an alternative to raw image patches, the input sequence can be formed from feature maps of a cnn (lecun et al., 1989). in this hybrid model,
the patch embedding projection E (eq.1) is applied to patches extracted from a cnn feature map.

fine-tuning and higher resolution
typically, we pre-train vit on large datasets, and fine-tune to smaller downstream tasks. for this, we remove the pre-trained prediction head 
and attach a zero_initialized DxK feedforward layer, where K is the number of downstream classes. it is often beneficial to fine-tune
at higher resolution than pre-training. when feeding iimages of higher resolution, we keep the patch size the same, which results in a 
larger effective sequence length. the vision transformer can handle arbitrary sequence lengths (up to memory constraints), however, 
the pre-trained position embeddings may nolonger be meaningful. 

model variants.
we base vit configurations on those used for BERT, as summarized in table 1. the base and large models are directly adopted from bert
and we add the large huge model. in what follows we use brief notation to indicate the model size and the input patch size: for instance,
vit-l/16 means the "large" variant with 16x16 input patch size. note that the transformer's sequence length is inversely proportional to the 
square of the patch size, thus models with smaller patch size are computationally more expensive.

{如果patch size很小，意味着一张图片被分割成了更多的small patches，就意味着模型的这个sequence包含了更多的tokens，因为无论我们的patch拥有什么样的size，它
都会需要通过linear projection layer变成固定维度的token vector，所以patch size越小，模型的就越computationally expensive}

for the baseline cnns, we use resnet, but replace the batch normalization layers with group normalization, and used standardized convs. these
modifications improve transfer, and we denote the modified model "resnet(bit)". for the hybrids, we feed the intermediate feature maps into
vit with patch size of one "pixel", to experiment with different sequence lengths, we either (i)

inspecting vision transformer
to begin to understand how the vision transformer processes image data, we analyze its internal representations. the first layer
of the vision transformer linearly projects the flattened patches into a lower-dimensional space
