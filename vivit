"vivit: a video vision transformer"

we present pure-transformer based models for video classification, drawing upon the recent
success of such models in image classfication. our model extracts spatio-temporal tokens
from the input video, which are then encoded by a series of transformer layers. 
in order to handle the long sequences of tokens encountered in video,

although transformer-based models are known to only be effective when large training datasets
are available, we show how we can effectively regularize the model during training and leverage
pretrained models are known to only be effective when large training datasets are available.
